containers
==========
data structure that permits storage & retrieval independent of content. e.g. stacks, queues. (as opposed to dicts etc. which do retrieval by key or value)

stacks and queues can be efficiently implimented by either arrays or linked lists -- generally if upper bound on size is known in advance you'd use an array

dictionaries
============
if implemented as an unsorted array, insert and delete are cheap, but search, succ/pred/min/max are O(n)
if sorted, insert and delete are O(n), but search is O(log(n)), and the other ops are O(1)

binary search trees
===================
root at top. each node has a value and up to two children. ordering: left child < parent < right child. even stonger: all transitive children of left child < parent < all transitive children of right child.

Given a binary tree with n nodes, and a set of n keys, there's a unique labelling to make a binary search tree.

To traverse, proceed recursively: search left, process centre, search right. guarantees that result is ordered

insertion: trivial; unique.
deletion: if leaf, trivial. If one child, replace with that. Otherwise replace with it's successor, i.e. the smallest value in its right subtree. This is guaranteed to be either a leaf or have one child, and will leave the rest of the tree still in order. (the largest value in the left subtree presumably works too, by symmetry)

If key insertion order is random, high likelyhood that tree will be pretty much balanced (i.e. height O(log(n)). But worst case is linear. There are 'balanced search tree' data structures that adjust after each insertion to guarantee O(log n)

priority queue
==============
queue where each item has a key which is its priority, and can efficiently find (and remove) the item with minimal or maximal priority, or insert with arbitrary priority.

Implementation: sorted or unsorted array or balanced tree, but keep a pointer to the minimal element, updating it on delete. Or a heap.

heap
====
maintains a partial order that's weaker than the sorted order, but stronger than random order.

Conceptually, a heap is a binary tree, where the root is at the top, and a node dominates (e.g. in a min-heap, 'is less than') its child nodes. In implementation, it's an array where the position of the keys replaces the role of pointers in a tree:
[root, 1st level left, 1st level right, 2ll, 2lr, 2rl, 2rr, ...]

so the parent of the nth node is in floor(n/2), it's children are in 2n and 2n+1.
(downside: sparse, elements still take up space if they're missing. and no efficient way to search for some particular key -- it's not a BST. But if all you want is to get and delete the dominating element, like for a briority queue or a heapsort, it's fine)

inserting an element: add to the leftmost empty position. check if it dominates its parent, if so, swap. repeat (i.e. bubble up). O(log(n))

removing the topmost element: remove the root. take the furthest right leaf and move it to the root, and bubble down. O(log(n))

heapsort: the removing the topmost element, appending it to an array. (== selection sort against a heap rather than a normal array)

faster way of constructing a heap: rather than repeatedly insert, just chuck everything into the heap and call 'bubble down' on the first half of the array. (the last half are all leaves, so are already 'trivial heaps'). This is more or less linear, rather than n log n.

Sorting
=======
General argument that all general comparison-based sorting algorithms are at least O(n log n): the sort must be able to distinguish every one of the n! different permutations. Each comparison can at most divide that possibility space in half (one bit of information). Then the number of comparisons necessary is log_2(n!) = \Theta(n log n)
n! = n*(n-1)*... >= n/2 ^ n/2
log_2(n!) = n/2 log_2(n/2)
= O(n log n)
